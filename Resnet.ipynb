{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":46149,"databundleVersionId":5450135,"sourceType":"competition"},{"sourceId":6118280,"sourceType":"datasetVersion","datasetId":3506742}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================================\n# FathomNet - ResNet50 Multilabel Classifier\n# With Focal Loss, Tunable Threshold, Per-Class Metrics,\n# Confusion Matrix visualization and sample predictions + t-SNE\n# ================================================================\n\nimport os\nimport json\nimport random\nimport math\nfrom collections import defaultdict, Counter\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\n\nfrom sklearn.metrics import (\n    f1_score,\n    precision_score,\n    recall_score,\n    accuracy_score,\n    multilabel_confusion_matrix,\n)\nimport seaborn as sns\n\n# -----------------------\n# CONFIG\n# -----------------------\nDATA_ROOT = \"/kaggle/input/fathomnet-out-of-sample-detection\"\nIMG_DIR = \"/kaggle/input/fathomnetimages/kaggle/working/images\"\nJSON_PATH = os.path.join(DATA_ROOT, \"object_detection/train.json\")\nTRAIN_CSV = os.path.join(DATA_ROOT, \"multilabel_classification/train.csv\")\nCATEGORY_KEY = os.path.join(DATA_ROOT, \"category_key.csv\")\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 8\nEPOCHS = 40\nLR = 1e-4\nIMG_SIZE = (224, 224)\nTHRESH = 0.5          # Tunable sigmoid probability threshold\nRANDOM_SEED = 42\nNUM_WORKERS = 2\nMODEL_SAVE_PATH = \"best_model_sampleiou.pt\"\n\nMEAN = [0.485, 0.456, 0.406]\nSTD  = [0.229, 0.224, 0.225]\n\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\n# -----------------------\n# Focal Loss Implementation\n# -----------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1.0, gamma=2.0, reduction=\"mean\"):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n\n    def forward(self, inputs, targets):\n        bce_loss = self.bce(inputs, targets)\n        probs = torch.sigmoid(inputs)\n        pt = torch.where(targets == 1, probs, 1 - probs)\n        focal_weight = self.alpha * (1 - pt) ** self.gamma\n        loss = focal_weight * bce_loss\n        if self.reduction == \"mean\":\n            return loss.mean()\n        elif self.reduction == \"sum\":\n            return loss.sum()\n        else:\n            return loss\n\n# -----------------------\n# Load category mapping\n# -----------------------\ncat_df = pd.read_csv(CATEGORY_KEY)\ncat_df[\"id\"] = cat_df[\"id\"].astype(int)\ncat_ids = cat_df[\"id\"].tolist()\ncat_id_to_idx = {cid: i for i, cid in enumerate(cat_ids)}\ncat_id_to_name = dict(zip(cat_df[\"id\"], cat_df[\"name\"]))\nNUM_CLASSES = len(cat_ids)\nprint(f\"Loaded {NUM_CLASSES} categories.\")\n\n# -----------------------\n# JSON + CSV linking\n# -----------------------\nwith open(JSON_PATH, \"r\") as f:\n    coco = json.load(f)\nimages_by_id = {img[\"id\"]: img for img in coco[\"images\"]}\n\nanns_by_stem = defaultdict(list)\nfor ann in coco[\"annotations\"]:\n    img_info = images_by_id.get(ann[\"image_id\"])\n    if img_info:\n        stem = os.path.splitext(img_info[\"file_name\"])[0]\n        anns_by_stem[stem].append(ann)\n\ndf = pd.read_csv(TRAIN_CSV)\ncsv_image_to_cats = {}\nfor _, row in df.iterrows():\n    stem = str(row[\"id\"]).strip()\n    cats = []\n    raw = row[\"categories\"]\n    if isinstance(raw, str) and raw.startswith(\"[\"):\n        parts = [x.strip() for x in raw.strip(\"[]\").split(\",\") if x.strip()]\n        try:\n            cats = [int(float(x)) for x in parts]\n        except:\n            cats = []\n    else:\n        try:\n            cats = [int(float(raw))]\n        except:\n            cats = []\n    csv_image_to_cats[stem] = cats\n\n# find stems present in CSV, JSON (anns), and directory\ndir_stems = {os.path.splitext(f)[0] for f in os.listdir(IMG_DIR) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))}\ncommon_stems = set(csv_image_to_cats.keys()) & set(anns_by_stem.keys()) & dir_stems\ncommon_list = sorted(list(common_stems))\nrandom.shuffle(common_list)\nsplit_idx = int(0.8 * len(common_list))\ntrain_stems = common_list[:split_idx]\nval_stems   = common_list[split_idx:]\n\nprint(f\"Total usable stems: {len(common_list)} | Train: {len(train_stems)} | Val: {len(val_stems)}\")\n\n# -----------------------\n# Dataset\n# -----------------------\nclass FathomNetDataset(Dataset):\n    def __init__(self, stems, img_dir, csv_map, anns_map, transform=None, img_size=IMG_SIZE):\n        self.stems = stems\n        self.img_dir = img_dir\n        self.csv_map = csv_map\n        self.anns_map = anns_map\n        self.transform = transform\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.stems)\n\n    def __getitem__(self, idx):\n        stem = self.stems[idx]\n        path = None\n        for ext in (\".png\", \".jpg\", \".jpeg\"):\n            p = os.path.join(self.img_dir, stem + ext)\n            if os.path.exists(p):\n                path = p\n                break\n        if path is None:\n            img = Image.new(\"RGB\", self.img_size, (0, 0, 0))\n        else:\n            img = Image.open(path).convert(\"RGB\")\n\n        if self.transform:\n            img = self.transform(img)\n\n        label = torch.zeros(NUM_CLASSES, dtype=torch.float32)\n        for c in self.csv_map.get(stem, []):\n            if c in cat_id_to_idx:\n                label[cat_id_to_idx[c]] = 1.0\n\n        return img, label, stem\n\n# transforms\ntrain_tfms = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(0.1, 0.1, 0.05),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD)\n])\n\nval_tfms = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD)\n])\n\n# create dataset objects (so we can reuse them later for visualization)\ntrain_dataset = FathomNetDataset(train_stems, IMG_DIR, csv_image_to_cats, anns_by_stem, train_tfms)\nval_dataset   = FathomNetDataset(val_stems,   IMG_DIR, csv_image_to_cats, anns_by_stem, val_tfms)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n)\n\n# -----------------------\n# Model\n# -----------------------\nmodel = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\nmodel.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\nmodel = model.to(DEVICE)\n\ncriterion = FocalLoss(alpha=1.0, gamma=2.0)\noptimizer = optim.AdamW(model.parameters(), lr=LR)\n\n# -----------------------\n# Utility helpers\n# -----------------------\ndef unnormalize_tensor(tensor, mean=MEAN, std=STD):\n    t = tensor.clone().cpu()\n    for c in range(3):\n        t[c] = t[c] * std[c] + mean[c]\n    t = torch.clamp(t, 0.0, 1.0)\n    return t\n\ndef tensor_to_numpy_img(tensor):\n    # tensor assumed CHW normalized\n    img = unnormalize_tensor(tensor)\n    img_np = img.permute(1, 2, 0).numpy()\n    return img_np\n\n# -----------------------\n# Evaluation helper\n# -----------------------\ndef evaluate(model, loader, thresh=THRESH):\n    model.eval()\n    all_true, all_pred = [], []\n    with torch.no_grad():\n        for imgs, labels, _ in loader:\n            imgs = imgs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            logits = model(imgs)\n            probs = torch.sigmoid(logits)\n            preds = (probs > thresh).float()\n            all_true.append(labels.cpu().numpy())\n            all_pred.append(preds.cpu().numpy())\n    if len(all_true) == 0:\n        return 0,0,0,0,0,np.zeros((0,NUM_CLASSES)),np.zeros((0,NUM_CLASSES)), np.zeros(NUM_CLASSES), np.zeros(NUM_CLASSES), np.zeros(NUM_CLASSES)\n    all_true = np.vstack(all_true)\n    all_pred = np.vstack(all_pred)\n\n    f1_micro = f1_score(all_true, all_pred, average=\"micro\", zero_division=0)\n    prec_micro = precision_score(all_true, all_pred, average=\"micro\", zero_division=0)\n    rec_micro = recall_score(all_true, all_pred, average=\"micro\", zero_division=0)\n\n    # per-class metrics\n    per_class_f1 = f1_score(all_true, all_pred, average=None, zero_division=0)\n    per_class_prec = precision_score(all_true, all_pred, average=None, zero_division=0)\n    per_class_rec = recall_score(all_true, all_pred, average=None, zero_division=0)\n\n    intersection = (all_true * all_pred).sum(axis=1)\n    union = ((all_true + all_pred) > 0).sum(axis=1)\n    sample_iou = np.mean(intersection / np.clip(union, a_min=1, a_max=None))\n    exact_acc = accuracy_score(all_true, all_pred)\n\n    return sample_iou, exact_acc, f1_micro, prec_micro, rec_micro, all_true, all_pred, per_class_f1, per_class_prec, per_class_rec\n\n# -----------------------\n# t-SNE helper (correct feature extraction + multi-label handling)\n# -----------------------\ndef extract_backbone_features(model, loader, device):\n    \"\"\"\n    Extracts backbone features (pre-FC) for all samples in loader.\n    Returns:\n      feats: (N, D) numpy array\n      labels: (N, NUM_CLASSES) numpy array (multi-hot)\n      stems: list of stems\n    \"\"\"\n    model.eval()\n    # build backbone extractor for ResNet-like model (everything up to the final avgpool)\n    # Using children approach is robust for torchvision ResNet\n    backbone = nn.Sequential(*list(model.children())[:-1]).to(device)\n    all_feats = []\n    all_labels = []\n    all_stems = []\n    with torch.no_grad():\n        for imgs, labels, stems in tqdm(loader, desc=\"Extracting backbone features\"):\n            imgs = imgs.to(device)\n            feats = backbone(imgs)    # shape: (B, 2048, 1, 1)\n            feats = feats.view(feats.size(0), -1)  # (B, 2048)\n            all_feats.append(feats.cpu())\n            all_labels.append(labels.cpu())\n            all_stems.extend(stems)\n    feats = torch.cat(all_feats, dim=0).numpy()\n    labels = torch.cat(all_labels, dim=0).numpy()\n    return feats, labels, all_stems\n\ndef compute_sample_primary_label_per_sample(labels, global_class_freq):\n    \"\"\"\n    labels: (N, C) multi-hot\n    global_class_freq: dict class->freq\n    For each sample, pick the primary label:\n      - if only one label present -> that\n      - if multiple -> choose the one with highest global freq (so that t-SNE groups by frequent classes)\n      - if none -> -1\n    Returns: array shape (N,) of primary label indices (or -1)\n    \"\"\"\n    primaries = []\n    for row in labels:\n        inds = np.where(row > 0.5)[0]\n        if len(inds) == 0:\n            primaries.append(-1)\n        elif len(inds) == 1:\n            primaries.append(inds[0])\n        else:\n            # choose the one with highest global frequency\n            best = max(inds, key=lambda x: global_class_freq.get(x, 0))\n            primaries.append(int(best))\n    return np.array(primaries, dtype=int)\n\ndef tsne_for_top_classes(model, loader, class_names, device, top_k=10, title_suffix=\"After Training\", max_points=2000, perplexity=30):\n    \"\"\"\n    Compute t-SNE for top_k frequent classes (global frequency over whole dataset).\n    Will plot only samples whose primary label (computed heuristically) is in top_classes.\n    \"\"\"\n    # first compute global class frequencies from the loader labels\n    # simpler: accumulate labels from loader (they are multi-hot)\n    all_labels = []\n    for imgs, labels, _ in loader:\n        all_labels.append(labels.numpy())\n    all_labels = np.vstack(all_labels) if len(all_labels) > 0 else np.zeros((0, NUM_CLASSES))\n    class_counts = dict(enumerate(all_labels.sum(axis=0).astype(int)))\n    top_classes = [c for c,_ in sorted(class_counts.items(), key=lambda x: -x[1])][:top_k]\n    if len(top_classes) == 0:\n        print(\"No classes found for t-SNE (empty dataset).\")\n        return\n\n    print(f\"Top-{top_k} classes by frequency: {[ (c, class_counts[c], class_names[c]) for c in top_classes ]}\")\n\n    # extract features\n    feats, labels, stems = extract_backbone_features(model, loader, device)\n    # compute primary label per sample\n    primaries = compute_sample_primary_label_per_sample(labels, class_counts)\n\n    # select samples whose primary label is in top_classes\n    mask = np.isin(primaries, top_classes)\n    feats_sel = feats[mask]\n    primaries_sel = primaries[mask]\n\n    if feats_sel.shape[0] == 0:\n        print(\"No selected samples for top classes (t-SNE).\")\n        return\n\n    # subsample if too many\n    if feats_sel.shape[0] > max_points:\n        rng = np.random.RandomState(RANDOM_SEED)\n        idxs = rng.choice(np.arange(feats_sel.shape[0]), size=max_points, replace=False)\n        feats_sel = feats_sel[idxs]\n        primaries_sel = primaries_sel[idxs]\n\n    # run t-SNE\n    tsne = TSNE(n_components=2, random_state=RANDOM_SEED, perplexity=min(perplexity, max(5, feats_sel.shape[0]//3)))\n    reduced = tsne.fit_transform(feats_sel)\n\n    plt.figure(figsize=(10, 8))\n    palette = sns.color_palette(\"tab10\", n_colors=len(top_classes))\n    label_to_color = {c: palette[i % len(palette)] for i, c in enumerate(top_classes)}\n    colors = [label_to_color[p] for p in primaries_sel]\n    sns.scatterplot(x=reduced[:,0], y=reduced[:,1], hue=[class_names[p] for p in primaries_sel],\n                    palette=[label_to_color[c] for c in top_classes], s=25, alpha=0.8)\n    plt.title(f\"t-SNE ({title_suffix}) for Top-{top_k} Frequent Classes\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    plt.tight_layout()\n    plt.show()\n\n# -----------------------\n# Sample Predictions Visualization\n# -----------------------\ndef visualize_sample_predictions_stacked(model, dataset, class_names, device, n_samples=20, thresh=THRESH):\n    \"\"\"\n    Shows n_samples images stacked vertically (1 per row) with GT and predicted labels.\n    \"\"\"\n    model.eval()\n    n = min(n_samples, len(dataset))\n    indices = random.sample(range(len(dataset)), n)\n    plt.figure(figsize=(8, n * 3))  # width x height: each image 3\" tall\n    with torch.no_grad():\n        for i, idx in enumerate(indices):\n            img_t, labels, stem = dataset[idx]  # img is normalized tensor\n            img_input = img_t.unsqueeze(0).to(device)\n            outputs = torch.sigmoid(model(img_input)).cpu().numpy().flatten()\n            preds = (outputs > thresh).astype(int)\n\n            gt_labels = [class_names[j] for j in range(len(labels)) if labels[j] == 1]\n            pred_labels = [class_names[j] for j in range(len(preds)) if preds[j] == 1]\n\n            img_np = tensor_to_numpy_img(img_t)  # unnormalized numpy HWC in [0,1]\n            ax = plt.subplot(n, 1, i + 1)\n            ax.imshow(img_np)\n            ax.axis(\"off\")\n            title = f\"{stem}  |  GT: {', '.join(gt_labels) if gt_labels else 'None'}\"\n            if pred_labels:\n                title += f\"  |  Pred (> {thresh:.2f}): {', '.join(pred_labels)}\"\n            else:\n                title += f\"  |  Pred (> {thresh:.2f}): None\"\n            ax.set_title(title, fontsize=9)\n    plt.tight_layout()\n    plt.show()\n\n# -----------------------\n# Training loop (save best by sample IoU)\n# -----------------------\nprint(\"Starting training...\\n\")\nbest_iou = -1.0\nhistory = []\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss = 0.0\n    for imgs, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n\n    train_loss = running_loss / max(1, len(train_loader.dataset))\n    sample_iou, exact_acc, f1, prec, rec, _, _, _, _, _ = evaluate(model, val_loader, thresh=THRESH)\n    history.append((train_loss, sample_iou, exact_acc, f1, prec, rec))\n\n    print(\n        f\"Epoch {epoch:02d} | Loss: {train_loss:.4f} | Accuracy: {sample_iou:.4f} | \"\n        f\"ExactAcc: {exact_acc:.4f} | F1: {f1:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f}\"\n    )\n\n    # Save best by sample IoU\n    if sample_iou > best_iou:\n        best_iou = sample_iou\n        best_state = model.state_dict().copy()\n        torch.save(best_state, MODEL_SAVE_PATH)\n        print(f\"✅ Saved new best model (Accuracy={best_iou:.4f}) → {MODEL_SAVE_PATH}\")\n\n# -----------------------\n# Final evaluation + confusion matrix + per-class metrics\n# -----------------------\nprint(\"\\nEvaluating Confusion Matrices...\")\nif os.path.exists(MODEL_SAVE_PATH):\n    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n    print(f\"Loaded best model from {MODEL_SAVE_PATH} (Accuracy={best_iou:.4f})\")\nelse:\n    print(\"No saved model found — using current model state for final eval.\")\n\nsample_iou, exact_acc, f1, prec, rec, y_true, y_pred, per_f1, per_prec, per_rec = evaluate(model, val_loader, thresh=THRESH)\nprint(f\"Final val: Accuracy={sample_iou:.4f} | ExactAcc={exact_acc:.4f} | F1={f1:.4f} | Prec={prec:.4f} | Rec={rec:.4f}\")\n\n# per-class metrics table\nprint(\"\\nTop-10 Classes by F1:\\n\")\ncls_metrics = pd.DataFrame({\n    \"ClassID\": cat_ids,\n    \"ClassName\": [cat_id_to_name[c] for c in cat_ids],\n    \"Precision\": per_prec,\n    \"Recall\": per_rec,\n    \"F1\": per_f1\n}).sort_values(\"F1\", ascending=False)\nprint(cls_metrics.head(10))\n\n# confusion matrices\ncm = multilabel_confusion_matrix(y_true, y_pred)\nn_plot = min(9, NUM_CLASSES)\ncols = 3\nrows = math.ceil(n_plot / cols)\nfig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\naxes = axes.flatten()\nfor i in range(n_plot):\n    ax = axes[i]\n    sns.heatmap(cm[i], annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, cbar=True)\n    ax.set_title(cat_id_to_name.get(cat_ids[i], f\"Class {cat_ids[i]}\"))\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\nfor j in range(n_plot, len(axes)):\n    axes[j].axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n# -----------------------\n# Sample Predictions (20 stacked)\n# -----------------------\nprint(\"\\nShowing 20 sample predictions (stacked vertically)...\")\nvisualize_sample_predictions_stacked(model, val_dataset, [cat_id_to_name[c] for c in cat_ids], DEVICE, n_samples=20, thresh=THRESH)\n\n# -----------------------\n# t-SNE: before and after training\n# -----------------------\n# Build a fresh untrained model for \"before training\" visualization (same architecture)\nprint(\"\\nComputing t-SNE BEFORE training (untrained weights)...\")\nuntrained_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\nuntrained_model.fc = nn.Linear(untrained_model.fc.in_features, NUM_CLASSES)\nuntrained_model = untrained_model.to(DEVICE)\ntsne_for_top_classes(untrained_model, val_loader, [cat_id_to_name[c] for c in cat_ids], DEVICE, top_k=10, title_suffix=\"Before Training\")\n\nprint(\"\\nComputing t-SNE AFTER training (best model)...\")\ntsne_for_top_classes(model, val_loader, [cat_id_to_name[c] for c in cat_ids], DEVICE, top_k=10, title_suffix=\"After Training\")\n\n# -----------------------\n# Epoch summary\n# -----------------------\nprint(\"\\nEpoch summary:\")\nfor e, (tloss, siou, acc, f1_e, prec_e, rec_e) in enumerate(history, start=1):\n    print(f\"Epoch {e:02d} | Loss: {tloss:.4f} | Accuracy: {siou:.4f} | ExactAcc: {acc:.4f} | \"\n          f\"F1: {f1_e:.4f} | Prec: {prec_e:.4f} | Rec: {rec_e:.4f}\")\n\nprint(f\"\\nBest model saved at: {MODEL_SAVE_PATH} (Accuracy={best_iou:.4f})\" if best_iou >= 0 else \"\\nNo best model saved.\")\nprint(\"\\nDone.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T15:19:25.242166Z","iopub.execute_input":"2025-10-30T15:19:25.242935Z","iopub.status.idle":"2025-10-30T15:22:05.968548Z","shell.execute_reply.started":"2025-10-30T15:19:25.242905Z","shell.execute_reply":"2025-10-30T15:22:05.967178Z"}},"outputs":[{"name":"stdout","text":"Loaded 290 categories.\nTotal usable stems: 5950 | Train: 4760 | Val: 1190\nStarting training...\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Loss: 0.0066 | Accuracy: 0.5119 | ExactAcc: 0.3992 | F1: 0.5503 | Prec: 0.6826 | Rec: 0.4609\n✅ Saved new best model (Accuracy=0.5119) → best_model_sampleiou.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/4198558111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{EPOCHS}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2}]}